# -*- coding: utf-8 -*-
"""AI-Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w_9FlDwK_XSn8UqCNCMbJT_mXIeJC6eC
"""

# from google.colab import drive
# drive.mount('/content/drive')

import math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud, STOPWORDS
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report
from numpy.random import rand
from numpy import log, dot, e
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import confusion_matrix
from sklearn import metrics
import functools
from sklearn.metrics import accuracy_score
import operator
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import GaussianNB

fake_data = pd.read_csv('Fake.csv')
true_data = pd.read_csv('True.csv')
fake_data.shape
true_data.shape

true_data['label'] = 1
fake_data['label'] = 0

data_frame = pd.concat([true_data, fake_data], ignore_index=True)

data_frame.drop(data_frame[data_frame.text ==
                           " "].index.tolist(), inplace=True)

data_frame.drop(data_frame[data_frame.duplicated()
                           ].index.tolist(), inplace=True)
data_frame
# We have 405 samples that match with each other in terms of title, text, date and subject, so we remove them.

data_frame.drop(['title', 'subject', 'date'], axis=1, inplace=True)
data_frame

data_frame.drop(data_frame[data_frame.duplicated(
    ['text'])].index.tolist(), inplace=True)
data_frame

# duplicated text remove

nltk.download('stopwords')

nltk_stopwords = stopwords.words('english')
wordcloud_stopwords = STOPWORDS

nltk_stopwords.extend(wordcloud_stopwords)

stopwords = set(nltk_stopwords)
print(stopwords)

"""Now we clean the title feature

We remove urls (if any)
Perform decontractions
Non-acronize few popular words
Remove punctuations and all special characters
Remove stopwords
"""


def clean(text):
    text = text.lower()
    # remove URLS
    text = re.sub("http\S+", '', str(text))

    # Punctuations & special characters
    text = re.sub("[^A-Za-z0-9]+", " ", str(text))
    text = re.sub(r"'s$", " is", str(text))
    text = re.sub(r"'re$", " are", str(text))
    text = re.sub(r"'t$", " not", str(text))
    text = re.sub(r"'ll$", " will", str(text))
    text = re.sub(r"'ve$", " have", str(text))
    text = re.sub(r"'d$", " would", str(text))
    text = re.sub('\n', '', text)

    # Stop word removal
    text = " ".join(str(i).lower()
                    for i in text.split() if i.lower() not in stopwords)

    return text


data_frame['text'] = data_frame['text'].apply(clean)

data_frame

"""#### Spliiting into train test"""

# # vectorizer = CountVectorizer(min_df=0.01,ngram_range=(1,3))
# # vectorizer.fit(X_train.text)

# # X_tr = vectorizer.transform(X_train.text)
# # X_te = vectorizer.transform(X_test.text)

# # print(X_tr.shape, X_te.shape)
# data_frame=data_frame.drop('label',axis=1)
# vectorizer = CountVectorizer()
# data_frame= vectorizer.fit_transform(data_frame)

X_train, X_test, y_train, y_test = train_test_split(data_frame.drop(
    'label', axis=1), data_frame.label, test_size=0.2, random_state=42)

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

vectorizer = TfidfVectorizer(min_df=0.01, ngram_range=(1, 3))
vectorizer.fit(X_train.text)

X_tr = vectorizer.transform(X_train.text)
X_te = vectorizer.transform(X_test.text)
# Y_tr = vectorizer.transform(y_train.text)
# Y_te = vectorizer.transform(y_test.text)
print(X_tr.shape, X_te.shape)

"""#### Model Training"""

logreg = LogisticRegression()
learnmodel = logreg.fit(X_tr, y_train)
lr_prediction = learnmodel.predict(X_te)
accuracy = accuracy_score(lr_prediction, y_test)
print("The accuracy score is :", accuracy)

"""#### Naive Bayes:"""

class_prob_dict = {}


def learn_model(data, target):

    # get list of all classes
    class_list = sorted(target.unique())
    encoded_doc = data.toarray()

    # find shape of the data
    def get_index(data, i): return data[i]
    data_row = get_index(encoded_doc.shape, 0)
    data_col = get_index(encoded_doc.shape, 1)

    # creating a 2d array of conditional probabilities
    conditioned_probabilities = np.array(
        [[0.0 for j in range(data_col)] for i in range(len(class_list))])

    # initializing probabilites with 0
    for i in range(len(class_list)):
        class_prob_dict[class_list[i]] = 0

    # populating class dictionary with its probabilities
    for i in range(len(class_list)):
        current = encoded_doc[target == class_list[i]]
        class_row = get_index(current.shape, 0)
        class_prob_dict[class_list[i]] = float(class_row/data_row)
        # populating the 2d array by finding conditional probabilities
        # laplace smoothing
        conditioned_probabilities[i] = (np.sum(current, axis=0) + 1)
        conditioned_probabilities[i] = conditioned_probabilities[i] / \
            np.sum(conditioned_probabilities[i])

    classifier = conditioned_probabilities
    # Your custom implementation of NaiveBayes classifier will go here
    return classifier


def classify(classifier, testdata):

    predicted_val = []
    # finding number of rows in testdata
    Test = testdata.toarray()
    def get_index(data, i): return data[i]
    test_row = get_index(Test.shape, 0)

    # initializing
    conditioned_probabilities = classifier
    class_prob = class_prob_dict.copy()
    classes = list(class_prob.keys())
    class_list_len = len(classes)

    for i in range(test_row):
        # initial probabilities of all classes is -1
        most_likely_class_list = [-1 for i in range(class_list_len)]
        for j in range(class_list_len):
            # finding probability of a particular class
            prob_class = class_prob[classes[j]]
            # finding all probabilites of Attribute given class
            prob_attr_given_class_list = conditioned_probabilities[j][Test[i] != 0]
            # multiplying all probabilities of Attribute given class
            prob_attr_given_class = functools.reduce(
                operator.mul, prob_attr_given_class_list, 1)
            # computing P(A|C)*P(C)
            most_likely_class_list[j] = float(prob_attr_given_class)*prob_class

        # selecting class with max probability
        most_likely_class = np.argmax(most_likely_class_list)
        predicted_val.append(classes[most_likely_class])

    return predicted_val


def evaluate(actual_class, predicted_class):

    # finding accuracy
    accuracy = -1
    accuracy = np.sum([actual_class == predicted_class]) / len(actual_class)
    actual_class = list(actual_class)

    # confusion Matrix
    cf = confusion_matrix(actual_class, predicted_class)

    # calculating TP, FP, FN and TN
    TP = []
    for i in range(len(cf)):
        for j in range(len(cf)):
            if (i == j):
                TP.append(cf[i][j])

    FP = []
    FN = []
    for i in range(len(cf)):
        calcFP = 0
        calcFN = 0
        for j in range(len(cf)):

            calcFP += cf[i][j]
            calcFN += cf[j][i]
        FP.append(calcFP)
        FN.append(calcFN)

    # precision
    # TP/TP+FP
    Precision = []

    # f_measure
    # 2*Precision*Recall/Precision+Recall
    Fmeasure = []

    # Recall
    # TP/TP+FN
    Recall = []

    for i in range(len(TP)):
        if TP[i] == 0:
            Precision.append(0)
            Recall.append(0)
            Fmeasure.append(0)
        else:
            calcP = TP[i]/FP[i]
            Precision.append(calcP)
            calcR = TP[i]/FN[i]
            Recall.append(calcR)
            calcFm = (2*calcP*calcR)/(calcP+calcR)
            Fmeasure.append(calcFm)

    print("The accuracy score is :", accuracy)
    print("The accuracy in % is: ", round(accuracy*100, 2), "%")
    print("The Precision is :", Precision)
    print("The Recall is :", Recall)
    print("The F_measure is :", Fmeasure)


print("Learning model.....")
model = learn_model(X_tr, y_train)

print("Classifying test data......")
predictedY = classify(model, X_te)

print("Evaluating results.....")
evaluate(y_test, predictedY)
# lr_prediction, y_test

logreg = MultinomialNB()
learnmodel = logreg.fit(X_tr, y_train)
lr_prediction = learnmodel.predict(X_te)
accuracy = accuracy_score(lr_prediction, y_test)
print("The accuracy score is :", accuracy)


def sigmoid(z):

    # calculate the sigmoid of z
    h = 1 / (1 + np.exp(-z))

    return h
